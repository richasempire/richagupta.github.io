<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>
      Red-Teaming Large Language Models for Implicit Bias Detection Richa |
      Portfolio
    </title>
    <link
      href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css"
      rel="stylesheet"
    />
    <link
      href="https://fonts.googleapis.com/css?family=Quicksand:300,400,500,700"
      rel="stylesheet"
    />
    <link
      href="https://fonts.googleapis.com/css?family=Playfair+Display:400,400i,700"
      rel="stylesheet"
    />
    <link
      href="//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700|Raleway:400,800,900"
      rel="stylesheet"
      async
    />
    <link rel="stylesheet" href="css/main.css" />
  </head>
  <body>
    <section class="d-block py-5 main_work">
      <div class="container-xl">
        <div class="row gy-5 align-items-center justify-content-between">
          <div class="col-md-12">
            <div
              class="d-flex justify-content-center align-items-center gap-4 flex-column"
            >
              <h1>
                Red-Teaming Large Language Models for Implicit Bias Detection
              </h1>
            </div>
          </div>
          <div class="col-md-8">
            <p class="objective">
              <strong>Objective : </strong>
              The goal of this assignment was to uncover implicit biases in
              GPT-4o through a systematic red-teaming approach. Biases were
              explored by analyzing the model’s responses to prompts involving
              human roles and behaviors.
            </p>
          </div>
          <div class="col-md-8 mx-auto">
            <div
              class="d-flex flex-md-row flex-column align-items-md-center gap-5 justify-content-between pt-3"
            >
              <!-- <a
                  target="_blank"
                  href="https://colab.research.google.com/drive/1OxX4qKHPNC9aoY-yISDRxw_8u_JEEWQT?usp=sharing"
                  >Click Here for Code</a
                > -->
            </div>
            <div class="d-flex flex-column gap-3">
              <h3>Introduction</h3>
              <p>
                Summary of my explorations in algorithmic fairness, bias
                detection, and ethical considerations within machine learning
                systems, completed as part of the coursework for AI,
                Decision-Making, and Society at MIT CSAIL. These problem focus
                on applying theoretical fairness frameworks, developing
                practical evaluations, and implementing mitigation strategies to
                address societal and ethical challenges posed by AI.
              </p>
              <div class="d-block buke-img pt-4">
                <img
                  class="w-100 h-100 object-fit-cover"
                  src="images/meth.jpg"
                  alt="bike"
                />
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="d-block py-5 main_work">
      <div class="container-xl">
        <div class="row gy-5 align-items-center justify-content-between">
          <div class="col-md-5">
            <div class="d-flex flex-column gap-3">
              <h3>Methodology</h3>
              <ul class="d-flex flex-column gap-4 ps-3">
                <li>
                  Designed a series of comparative prompts to evaluate GPT-4o’s
                  characterization of individuals based on beverage preferences
                  (e.g., coffee vs. tea).
                </li>
                <li>
                  Evaluated responses across multiple contexts, including
                  personality traits, work ethics, adaptability under pressure,
                  and life purpose
                </li>
                <li>
                  Analyzed latent stereotypes by examining lexical choices,
                  sentiment, and role-specific recommendations.
                </li>
              </ul>
            </div>
          </div>
          <div class="col-md-6">
            <div class="d-block buke-img">
              <img
                class="w-100 h-100 object-fit-cover"
                src="images/blog-4.jpg"
                alt="bike"
              />
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="d-block py-5 main_work">
      <div class="container-xl">
        <div class="row gy-5 align-items-center justify-content-between">
          <div class="col-md-5">
            <div class="d-flex flex-column gap-3">
              <h3>Findings</h3>
              <p>
                The results revealed significant discrepancies in descriptions
                based on beverage preference:
              </p>
              <ul class="d-flex flex-column gap-3">
                <li class="list-none">
                  <strong>Coffee Drinkers :</strong> Portrayed as "driven,"
                  "energetic," and prone to burnout, with traits aligned toward
                  high-paced and leadership roles.
                </li>
                <li class="list-none">
                  <strong>Tea Drinkers :</strong> Described as "calm,"
                  "balanced," and reflective, with traits favoring harmony,
                  thoughtfulness, and collaboration. These findings highlight
                  the presence of subtle cultural stereotypes embedded in the
                  language model.
                </li>
              </ul>
              <div class="d-block buke-img pt-4">
                <img
                  class="w-100 h-100 object-fit-cover"
                  src="images/img_bg_2.jpg"
                  alt="bike"
                />
              </div>
            </div>
          </div>
          <div class="col-md-6">
            <div class="dataset-overview">
              <div class="d-flex flex-column gap-4">
                <div class="d-flex flex-column gap-3">
                  <h3>Impact</h3>
                  <p>
                    This red-teaming approach demonstrated the utility of
                    structured bias evaluations in LLMs. It emphasized the
                    importance of uncovering implicit assumptions to ensure fair
                    and representative outputs across diverse groups.
                  </p>
                  <p>
                    Here’s a complete set of tables summarizing each PSET in a
                    clear, academic style. These tables provide structured
                    insights into the objectives, methodologies, findings, and
                    impacts for each problem set. You can include them in your
                    document for clarity and conciseness.
                  </p>
                </div>
                <div class="d-flex flex-column gap-3">
                  <table>
                    <thead>
                      <tr>
                        <th>Section</th>
                        <th>Details</th>
                      </tr>
                    </thead>
                    <tbody>
                      <tr>
                        <td><strong>Objective</strong></td>
                        <td>
                          Identify and analyze implicit biases in GPT-4o through
                          red-teaming.
                        </td>
                      </tr>
                      <tr>
                        <td><strong>Methodology</strong></td>
                        <td>
                          - Developed comparative prompts evaluating
                          personality, work habits, and life purpose.<br />
                          - Analyzed latent stereotypes in LLM outputs.
                        </td>
                      </tr>
                      <tr>
                        <td><strong>Findings</strong></td>
                        <td>
                          - <strong>Coffee drinkers:</strong> Described as
                          "driven" and "energetic" but prone to burnout.<br />
                          - <strong>Tea drinkers:</strong> Described as "calm,"
                          "balanced," and reflective.
                        </td>
                      </tr>
                      <tr>
                        <td><strong>Impact</strong></td>
                        <td>
                          Demonstrated structured red-teaming to detect latent
                          stereotypes, informing fairness-aware AI design.
                        </td>
                      </tr>
                    </tbody>
                  </table>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="py-md-5">
      <div class="container-xl">
        <p class="text-center">
          <strong>Table : </strong>Red-Teaming Large Language Models for Bias
          Detection
        </p>
      </div>
    </section>
    <section class="d-block py-5">
      <div class="container-xl pb-5">
        <div class="methodology">
          <div class="row gy-4 align-items-center">
            <div class="col-md-8 mx-auto">
              <div
                class="d-flex flex-column justify-content-center align-items-center gap-4"
              >
                <h1 class="text-center">Conclusion</h1>
                <p>
                  Through this and other problems like these, I applied fairness
                  frameworks, bias detection methods, and privacy-preserving
                  strategies to evaluate and address ethical challenges in AI
                  systems. These works demonstrate my ability to design rigorous
                  evaluations, identify systemic biases, and implement solutions
                  that align with societal values. Such methodologies are
                  essential for building AI systems that are fair, responsible,
                  and impactful.
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"></script>
  </body>
</html>
