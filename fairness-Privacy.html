<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>
      Structural Fairness and Privacy in Machine Learning Objective Richa |
      Portfolio
    </title>
    <link
      href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css"
      rel="stylesheet"
    />
    <link
      href="https://fonts.googleapis.com/css?family=Quicksand:300,400,500,700"
      rel="stylesheet"
    />
    <link
      href="https://fonts.googleapis.com/css?family=Playfair+Display:400,400i,700"
      rel="stylesheet"
    />
    <link
      href="//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700|Raleway:400,800,900"
      rel="stylesheet"
      async
    />
    <link rel="stylesheet" href="css/main.css" />
  </head>
  <body>
    <section class="d-block py-5 main_work">
      <div class="container-xl">
        <div class="row gy-5 align-items-center justify-content-between">
          <div class="col-12">
            <div
              class="d-flex justify-content-center align-items-center gap-4 flex-column"
            >
              <h1>
                Structural Fairness and Privacy in Machine Learning Objective
              </h1>
            </div>
          </div>
          <div class="col-md-8 me-auto">
            <p class="sub-head">
              This problem set examined formal fairness frameworks and
              privacy-preserving mechanisms to address algorithmic bias in
              decision-making systems.
            </p>
          </div>
          <div class="col-md-5 pt-5">
            <div class="d-flex flex-column gap-3">
              <h3>Introduction</h3>
              <p>
                Summary of my explorations in algorithmic fairness, bias
                detection, and ethical considerations within machine learning
                systems, completed as part of the coursework for AI,
                Decision-Making, and Society at MIT CSAIL. These problem focus
                on applying theoretical fairness frameworks, developing
                practical evaluations, and implementing mitigation strategies to
                address societal and ethical challenges posed by AI.
              </p>
            </div>
          </div>
          <div class="col-md-6">
            <div class="d-block buke-img">
              <img
                class="w-100 h-100 object-fit-cover"
                src="images/img_bg_2.jpg"
                alt="bike"
              />
            </div>
          </div>
        </div>
      </div>
    </section>
    <section class="d-block py-5 main_work">
      <div class="w-100">
        <div class="row gy-5 align-items-center">
          <div class="col-md-12">
            <div class="d-block buke-img">
              <img
                class="w-100 h-100 object-fit-cover"
                src="images/meth2.jpg"
                alt="bike"
              />
            </div>
          </div>
        </div>
      </div>
    </section>
    <section class="d-block py-5 main_work">
      <div class="container-xl">
        <div class="row gy-5 align-items-center justify-content-between">
          <div class="col-md-3">
            <div class="d-block buke-img">
              <img
                class="w-100 h-100 object-fit-cover"
                src="images/cancu.png"
                alt="bike"
              />
            </div>
          </div>
          <div class="col-md-4 pb-5">
            <div class="d-flex flex-column gap-3">
              <h3>Methodology</h3>
              <ul class="d-flex flex-column gap-4 ps-3">
                <li>
                  <strong>Differential Privacy :</strong>
                  Implemented a privacy-preserving mechanism on census data to
                  mitigate risks of reconstruction attacks.
                </li>
                <li>
                  <strong>Fairness Trade-offs : </strong> Evaluated the impact
                  of fairness constraints on model performance across sensitive
                  demographic groups.
                </li>
                <li>
                  <strong> Structural Fairness : </strong> Analyzed the role of
                  data representation in perpetuating systemic disparities.
                </li>
              </ul>
            </div>
          </div>
          <div class="col-md-3">
            <div class="d-block buke-img">
              <img
                class="w-100 h-100 object-fit-cover"
                src="images/meth.jpg"
                alt="bike"
              />
            </div>
          </div>
        </div>
      </div>
    </section>
    <section class="d-block py-5 main_work">
      <div class="container-xl">
        <div class="row gy-5 align-items-center justify-content-between">
          <div class="col-md-6">
            <div class="d-block buke-img">
              <img
                class="w-100 h-100 object-fit-cover"
                src="images/blog-3.jpg"
                alt="bike"
              />
            </div>
          </div>
          <div class="col-md-5">
            <div class="d-flex flex-column gap-3">
              <h3>Findings</h3>

              <ul class="d-flex flex-column gap-3">
                <li class="list-none">
                  <strong>Privacy Trade-offs : </strong> Adding differential
                  privacy successfully reduced reconstruction risks but
                  introduced marginal decreases in data utility.
                </li>
                <li class="list-none">
                  <strong>Fairness Constraints : </strong>Enforcing fairness
                  constraints improved outcomes for underrepresented groups but
                  required careful balance to avoid significant accuracy
                  trade-offs.
                </li>
                <li class="list-none">
                  <strong>Structural Bias : </strong> Highlighted the systemic
                  nature of biases arising from skewed data distributions.
                </li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </section>
    <section class="py-5">
      <div class="container-xl">
        <div class="row align-items-center justify-content-between">
          <div class="col-md-7">
            <div class="d-flex flex-column gap-3 pb-4 main_work">
              <h3>Impact</h3>
              <p>
                This work underscores the importance of balancing fairness,
                accuracy, and privacy in AI systems. By leveraging differential
                privacy and fairness metrics, it provides practical solutions
                for mitigating algorithmic risks.
              </p>
            </div>

            <div class="dataset-overview">
              <div class="d-flex flex-column gap-4">
                <div class="d-flex flex-column gap-3">
                  <table>
                    <thead>
                      <tr>
                        <th>Section</th>
                        <th>Details</th>
                      </tr>
                    </thead>
                    <tbody>
                      <tr>
                        <td><strong>Objective</strong></td>
                        <td>
                          Implement fairness trade-offs and privacy-preserving
                          mechanisms in ML systems.
                        </td>
                      </tr>
                      <tr>
                        <td><strong>Methodology</strong></td>
                        <td>
                          - Applied differential privacy to mitigate
                          reconstruction risks in census data.<br />
                          - Evaluated fairness constraints to improve outcomes
                          for sensitive demographic groups.
                        </td>
                      </tr>
                      <tr>
                        <td><strong>Findings</strong></td>
                        <td>
                          - Differential privacy reduced reconstruction risks
                          but introduced minor trade-offs in data utility.<br />
                          - Fairness constraints mitigated group disparities but
                          required balancing accuracy and fairness.
                        </td>
                      </tr>
                      <tr>
                        <td><strong>Impact</strong></td>
                        <td>
                          Demonstrated practical solutions for achieving
                          fairness and privacy in machine learning systems.
                        </td>
                      </tr>
                    </tbody>
                  </table>
                </div>
              </div>
            </div>
          </div>
          <div class="col-md-4">
            <div class="d-block buke-img pt-4">
              <img
                class="w-100 h-100 object-fit-cover"
                src="images/blog-1.jpg"
                alt="bike"
              />
            </div>
            <p class="pt-5">
              <strong>Table : </strong>PSET 6 – Structural Fairness and
              Differential Privacy in ML
            </p>
          </div>
        </div>
      </div>
    </section>
    <section class="d-block py-5">
      <div class="w-100">
        <div class="methodology">
          <div class="row gy-5 align-items-center">
            <div class="col-12">
              <div class="d-block canva-img">
                <img
                  class="w-100 h-100 object-fit-cover"
                  src="images/canva.jpg"
                  alt="bike"
                />
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <section class="py-5">
      <div class="container-xl">
        <div class="main_work">
          <div class="row gy-5 align-items-center justify-content-between">
            <div class="col-md-4 pt-5">
              <div class="d-block buke-img pt-md-4">
                <img
                  class="w-100 h-100 pt-md-5 object-fit-cover"
                  src="images/blog-4.jpg"
                  alt="bike"
                />
              </div>
              <p class="pt-5">
                <strong>Table : </strong>PSET 6 – Structural Fairness and
                Differential Privacy in ML
              </p>
            </div>
            <div class="col-md-7">
              <div class="dataset-overview">
                <div class="d-flex flex-column gap-4">
                  <div class="d-flex flex-column gap-3">
                    <h3 class="pb-4">Additional work</h3>

                    <table>
                      <thead>
                        <tr>
                          <th>Section</th>
                          <th>Details</th>
                        </tr>
                      </thead>
                      <tbody>
                        <tr>
                          <td><strong>Objective</strong></td>
                          <td>
                            Implement fairness trade-offs and privacy-preserving
                            mechanisms in ML systems.
                          </td>
                        </tr>
                        <tr>
                          <td><strong>Methodology</strong></td>
                          <td>
                            - Applied differential privacy to mitigate
                            reconstruction risks in census data.<br />
                            - Evaluated fairness constraints to improve outcomes
                            for sensitive demographic groups.
                          </td>
                        </tr>
                        <tr>
                          <td><strong>Findings</strong></td>
                          <td>
                            - Differential privacy reduced reconstruction risks
                            but introduced minor trade-offs in data utility.<br />
                            - Fairness constraints mitigated group disparities
                            but required balancing accuracy and fairness.
                          </td>
                        </tr>
                        <tr>
                          <td><strong>Impact</strong></td>
                          <td>
                            Demonstrated practical solutions for achieving
                            fairness and privacy in machine learning systems.
                          </td>
                        </tr>
                      </tbody>
                    </table>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>

        <p class="pt-5 text-end">
          <strong>Table : </strong>Exploratory Data Analysis and Prompt
          Engineering
        </p>
      </div>
    </section>

    <section class="pb-5">
      <div class="container-xl">
        <div class="row align-content-center justify-content-between">
          <div class="col-md-7">
            <div class="dataset-overview">
              <div class="d-flex flex-column gap-4">
                <div class="d-flex flex-column gap-3">
                  <table>
                    <thead>
                      <tr>
                        <th>Section</th>
                        <th>Details</th>
                      </tr>
                    </thead>
                    <tbody>
                      <tr>
                        <td><strong>Objective</strong></td>
                        <td>
                          Evaluate recommender systems, content moderation, and
                          misinformation on online platforms.
                        </td>
                      </tr>
                      <tr>
                        <td><strong>Methodology</strong></td>
                        <td>
                          - Built K-NN-based recommendation models. <br />
                          - Analyzed user engagement metrics and content
                          patterns.
                        </td>
                      </tr>
                      <tr>
                        <td><strong>Findings</strong></td>
                        <td>
                          - Trade-off between accuracy (high ratings) and
                          popularity (accumulated ratings). <br />
                          - Highlighted systemic biases in recommendation
                          algorithms and misinformation propagation risks.
                        </td>
                      </tr>
                    </tbody>
                  </table>
                </div>
              </div>
            </div>
            <p class="pt-5">
              <strong>Impact : </strong>Showcased the societal implications of
              algorithmic decisions and user interaction models.
            </p>
            <p class="pt-3">
              <strong>Table : </strong>Algorithmic Impact on Online Platforms
            </p>
          </div>
          <div class="col-md-4">
            <div class="h-100 d-flex justify-content-center align-items-center">
              <div class="d-block buke-img pt-4">
                <img
                  class="w-100 h-100 object-fit-cover"
                  src="images/meth1.jpg"
                  alt="bike"
                />
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <section class="d-block py-5">
      <div class="w-100">
        <div class="methodology">
          <div class="row gy-5 align-items-center">
            <div class="col-12">
              <div class="d-block xcab-img">
                <img
                  class="w-100 h-100 object-fit-cover"
                  src="images/cancu.png"
                  alt="bike"
                />
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>
    <section class="d-block py-5">
      <div class="container-xl pb-5">
        <div class="methodology">
          <div class="row gy-5 align-items-center">
            <div class="col-md-8 mx-auto">
              <div class="d-flex flex-column gap-3">
                <h1 class="text-center">Conclusion</h1>
                <p>
                  Through this and other problems like these, I applied fairness
                  frameworks, bias detection methods, and privacy-preserving
                  strategies to evaluate and address ethical challenges in AI
                  systems. These works demonstrate my ability to design rigorous
                  evaluations, identify systemic biases, and implement solutions
                  that align with societal values. Such methodologies are
                  essential for building AI systems that are fair, responsible,
                  and impactful.
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"></script>
  </body>
</html>
