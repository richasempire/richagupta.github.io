<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>
      Gender Bias Evaluations in Job Descriptions and Role Representation Richa
      | Portfolio
    </title>
    <link
      href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css"
      rel="stylesheet"
    />
    <link
      href="https://fonts.googleapis.com/css?family=Quicksand:300,400,500,700"
      rel="stylesheet"
    />
    <link
      href="https://fonts.googleapis.com/css?family=Playfair+Display:400,400i,700"
      rel="stylesheet"
    />
    <link
      href="//fonts.googleapis.com/css?family=Source+Sans+Pro:400,700|Raleway:400,800,900"
      rel="stylesheet"
      async
    />
    <link rel="stylesheet" href="css/main.css" />
  </head>
  <body>
    <section class="d-block py-5 main_work">
      <div class="container-xl">
        <div class="row gy-5">
          <div class="col-12">
            <div
              class="d-flex justify-content-center align-items-center gap-4 flex-column"
            >
              <h1>
                Gender Bias Evaluations in Job Descriptions and Role
                Representation
              </h1>
            </div>
          </div>
          <div class="col-md-8 ms-auto">
            <p class="objective pb-5 pt-3">
              <strong>Objective : </strong>
              This problem set aimed to design evaluations for gender biases in
              LLM outputs, particularly focusing on professions historically
              associated with men and women.
            </p>
          </div>
          <div class="col-md-7 mx-auto">
            <div class="d-flex flex-column gap-3">
              <h3>Introduction</h3>
              <p>
                Summary of my explorations in algorithmic fairness, bias
                detection, and ethical considerations within machine learning
                systems, completed as part of the coursework for AI,
                Decision-Making, and Society at MIT CSAIL. These problem focus
                on applying theoretical fairness frameworks, developing
                practical evaluations, and implementing mitigation strategies to
                address societal and ethical challenges posed by AI.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="d-block py-5 main_work">
      <div class="w-100">
        <div class="row gy-5">
          <div class="col-12">
            <div class="d-block buke-img grnderback">
              <img
                class="w-100 h-100 object-fit-cover"
                src="images/meth1.jpg"
                alt="bike"
              />
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="d-block py-5 main_work">
      <div class="container-xl">
        <div class="row gy-5 align-items-center justify-content-between">
          <div class="col-md-6 pe-md-5">
            <div class="d-block buke-img">
              <img
                class="w-100 h-100 object-fit-cover"
                src="images/img_bg_2.jpg"
                alt="bike"
              />
            </div>
          </div>
          <div class="col-md-6 ps-md-5">
            <div class="d-flex flex-column gap-3">
              <h3>Methodology</h3>
              <ul class="d-flex flex-column gap-4 ps-3">
                <li>
                  Developed lists of professions traditionally dominated by each
                  gender, ensuring representation of leadership, technical, and
                  nurturing roles.
                </li>
                <li>
                  Created prompts to elicit model responses that could reveal
                  biases in role descriptions and inspirational role models for
                  each profession.
                </li>
                <li>
                  Analyzed outputs for differences in word choice, leadership
                  attributes, and value associations.
                </li>
              </ul>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="d-block py-5 main_work">
      <div class="container-xl">
        <div class="row gy-5">
          <div class="col-md-7 mx-auto">
            <div class="d-flex flex-column gap-3">
              <h3>Findings</h3>

              <ul class="d-flex flex-column gap-3">
                <li class="list-none">
                  <strong>Male-Associated Roles :</strong> Responses emphasized
                  leadership, technical expertise, and innovation, often citing
                  figures like Steve Jobs or Elon Musk.
                </li>
                <li class="list-none">
                  <strong>Female-Associated Roles :</strong> Responses
                  emphasized care, support, and dedication, referencing figures
                  like Florence Nightingale.
                </li>
              </ul>
              <p>
                The model displayed a bias towards reinforcing traditional
                gender stereotypes, underscoring the need for targeted fairness
                evaluations.
              </p>
            </div>
          </div>
        </div>
      </div>
    </section>

    <section class="py-5">
      <div class="container-xl">
        <div class="row align-items-center justify-content-between">
          <div class="col-md-7">
            <div class="dataset-overview">
              <div class="d-flex flex-column gap-4">
                <div class="d-flex flex-column gap-3">
                  <h1>Impact</h1>
                  <p>
                    This evaluation framework provides a systematic approach to
                    uncovering and quantifying gender biases in AI systems. Such
                    work is critical for ensuring equitable representation
                    across all professional domains.
                  </p>
                </div>
                <div class="d-flex flex-column gap-3">
                  <table>
                    <thead>
                      <tr>
                        <th>Section</th>
                        <th>Details</th>
                      </tr>
                    </thead>
                    <tbody>
                      <tr>
                        <td><strong>Objective</strong></td>
                        <td>
                          Design evaluations to detect gender biases in job
                          descriptions and role models generated by LLMs.
                        </td>
                      </tr>
                      <tr>
                        <td><strong>Methodology</strong></td>
                        <td>
                          - Created prompts for professions historically
                          associated with men and women.<br />
                          - Analyzed LLM responses for leadership traits,
                          stereotypes, and gender-specific references.
                        </td>
                      </tr>
                      <tr>
                        <td><strong>Findings</strong></td>
                        <td>
                          - <strong>Male-associated jobs:</strong> Highlighted
                          leadership and technical attributes (e.g., Steve
                          Jobs).<br />
                          - <strong>Female-associated jobs:</strong> Emphasized
                          nurturing and supportive roles (e.g., Florence
                          Nightingale).
                        </td>
                      </tr>
                      <tr>
                        <td><strong>Impact</strong></td>
                        <td>
                          Provided a systematic framework for identifying and
                          addressing gender biases in AI systems.
                        </td>
                      </tr>
                    </tbody>
                  </table>
                </div>
              </div>
            </div>
            <p class="pt-5 pb-md-0 pb-5">
              <strong>Table : </strong>Gender Bias Evaluations in Job
              Descriptions.
            </p>
          </div>
          <div class="col-md-4">
            <div class="d-block buke-img">
              <img
                class="w-100 h-100 object-fit-cover"
                src="images/blog-4.jpg"
                alt="bike"
              />
            </div>
          </div>
        </div>
      </div>
    </section>
    <section class="d-block py-5">
      <div class="container-xl pb-5">
        <div class="methodology">
          <div class="row gy-4 align-items-center">
            <div class="col-md-8 mx-auto">
              <div class="d-flex flex-column gap-3">
                <h1 class="text-center">Conclusion</h1>
                <p>
                  Through this and other problems like these, I applied fairness
                  frameworks, bias detection methods, and privacy-preserving
                  strategies to evaluate and address ethical challenges in AI
                  systems. These works demonstrate my ability to design rigorous
                  evaluations, identify systemic biases, and implement solutions
                  that align with societal values. Such methodologies are
                  essential for building AI systems that are fair, responsible,
                  and impactful.
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </section>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"></script>
  </body>
</html>
